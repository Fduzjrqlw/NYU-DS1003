{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangjunrui/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset...\n"
     ]
    }
   ],
   "source": [
    "#读入数据集\n",
    "print ('loading the dataset...')\n",
    "df = pd.read_csv('hw1-data.csv' , delimiter = ',')\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[: ,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into Train and Test...\n"
     ]
    }
   ],
   "source": [
    "#划分训练集和测试集\n",
    "print ('Split into Train and Test...')\n",
    "# train_test_split 参数介绍 \n",
    "## train_test_split(train_data , test_data , test_size , random_state) \n",
    "## 其中test_size可选择正整数或者是[0,1]中的浮点数,表示测试集大小\n",
    "## random_state为随机数种子编号，方便调试\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 100 , random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_normalization(train , test) : \n",
    "    '''\n",
    "    将每一维特征的数据利用线性变换[0,1]对训练数据和测试数据归一化,其中统计量来自于训练数据.\n",
    "    Args:\n",
    "        train - training set , 二维numpy数组(num_instances , num_features)\n",
    "        test - test set , 二维numpy数组(num_instances , num_features)\n",
    "    \n",
    "    Returns:\n",
    "        train_normalized - training set after normalization \n",
    "        test_normalized - test set after normalization\n",
    "    '''\n",
    "    mean = np.mean(train , axis = 0)\n",
    "    std = np.std(train , axis = 0)\n",
    "    try :\n",
    "        Max = np.max(train , axis = 0)\n",
    "        Min = np.min(train , axis = 0)\n",
    "        #这个地方的减法运用了广播机制,考虑后缘维度(从末尾开始算起的维度)的轴长度相同,或者其中的一方长度为1.\n",
    "        train_normalized = (train - Min) / (Max - Min)\n",
    "        test_normalized = (test - Min) / (Max - Min)\n",
    "    except ValueError :\n",
    "        print ('There maybe some features are exactly same !')\n",
    "    return train_normalized , test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling all to [0,1]\n"
     ]
    }
   ],
   "source": [
    "#归一化\n",
    "print ('Scaling all to [0,1]')\n",
    "X_train , X_test = feature_normalization(X_train , X_test)\n",
    "#添加偏置项 Add bias term\n",
    "## np.hstack函数用来横向堆叠   np.hstack([arr1 , arr2])   要求arr1和arr2都是相同维数的矩阵,比如二维矩阵.然后其中行数要相同\n",
    "## np.vstack函数用来纵向堆叠\n",
    "X_train = np.hstack((X_train , np.ones([X_train.shape[0] , 1])))\n",
    "X_test = np.hstack((X_test , np.ones([X_test.shape[0] , 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_square_loss(X , y , theta) :\n",
    "    '''\n",
    "    给定X,y,theta,计算用X*theta作为y预测的平方损失   loss = |X * theta - y|^2 / m\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "    Returns:\n",
    "        loss - 平方损失 , 标量\n",
    "    '''\n",
    "    #这里用到了np.matmul的一些性质 y_ = X * theta.T\n",
    "    #np.matmul(X,theta) = np.matmul(X,theta.T)\n",
    "    y_ = np.matmul(X , theta)\n",
    "    #print (y_)\n",
    "    loss = np.mean((y_ - y) ** 2) / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_square_loss_gradient(X , y , theta) :\n",
    "    ''' \n",
    "    计算平方损失函数关于参数theta的梯度   grad = X^T * (X * theta - y) / m\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "    Returns:\n",
    "        grad - 梯度向量 , 一维numpy数组(num_features)\n",
    "    '''\n",
    "    num_instances = X.shape[0]\n",
    "    grad = np.matmul(X.T  , np.matmul(X , theta) - y) / num_instances\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.ones(X_train.shape[1])\n",
    "loss = compute_square_loss(X_train , y_train , theta)\n",
    "grad = compute_square_loss_gradient(X_train , y_train , theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 32]\n",
      "[14 32]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "w = np.array([1,2,3])\n",
    "print (np.matmul(x , w))\n",
    "print (np.matmul(x , w.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check(X , y , theta , epsilon = 0.01 , tolerance = 1e-4) :\n",
    "    '''\n",
    "    利用梯度的定义近似计算梯度，以检验compute_square_loss_gradient函数计算的梯度是否正确\n",
    "    设d表示特征维度(包括了bias term),e_1=(1,0,...,0),e_2=(0,1,0,...,0),...,e_d=(0,0,...,0,1)\n",
    "    近似梯度可以通过如下公式计算,其第i个分量可以表示为\n",
    "        J(theta + epsilon * e_i) - J(theta - epsilon * e_i) / (2 * epsilon)\n",
    "    之后考虑计算真实梯度和近似梯度的欧几里得距离来验证计算是否正确\n",
    "    \n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        epsilon - 近似时使用的单位方向上的步长参数\n",
    "        tolerance - 允许误差\n",
    "    \n",
    "    Returns:\n",
    "        一个布尔值 , 表示计算是否正确\n",
    "    '''\n",
    "    true_grad = compute_square_loss_gradient(X , y , theta)\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    I = np.identity(num_features)\n",
    "    for i in range(num_features) :\n",
    "        delta = I[i] * epsilon\n",
    "        approx_grad[i] = (compute_square_loss(X , y , theta + delta) - compute_square_loss(X , y , theta - delta))/ (2 * epsilon)\n",
    "    dis = np.mean((true_grad - approx_grad)**2)\n",
    "    return (dis < tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_check(X_train , y_train , theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generic_gradient_check(X , y , theta , objective_func , gradient_func , epsilon = 0.01 , tolerance = 1e-4) :\n",
    "    '''\n",
    "    一般情形的梯度检验函数\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        objective_func - 目标函数的函数,返回目标值loss\n",
    "        gradient_func - 梯度公式函数,返回梯度\n",
    "        epsilon - 近似时使用的单位方向上的步长参数\n",
    "        tolerance - 允许误差\n",
    "    Returns:\n",
    "        一个布尔值 , 表示计算是否正确\n",
    "    '''\n",
    "    true_grad = gradient_func(X , y , theta)\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    ind = np.identity(num_features)\n",
    "    for i in range(num_features) :\n",
    "        delta = ind[i]\n",
    "        approx_grad[i] = (objective_func(X , y , theta + epsilon * delta) - objective_func(X , y , theta - epsilon * delta)) / (2 * epsilon)\n",
    "    dis = np.mean((true_grad - approx_grad)**2)\n",
    "    return (dis < tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_grad_descent(X , y , alpha = 0.01 , num_iter = 1000 , check_gradient = False) :\n",
    "    '''\n",
    "    利用梯度下降法求解平方损失函数的线性模型的参数\n",
    "    \n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        alpha - 学习率/梯度步长 , 标量\n",
    "        num_iter - 最大迭代次数 , 标量\n",
    "        check_gradient - 是否进行梯度计算检验 , 布尔值\n",
    "    Returns:\n",
    "        theta_hist - 迭代过程中储存的参数列表 , 二维numpy数组(num_iter + 1 , num_features) , 其中 theta_hist[0]为初始参数, \n",
    "                     theta_hist[-1]为迭代结束时的参数\n",
    "        loss_hist - 迭代过程中储存的loss列表,一维numpy数组(num_iter + 1)\n",
    "    '''\n",
    "    [num_instances , num_features] = X.shape\n",
    "    theta_hist = np.zeros((num_iter + 1 , num_features))  #初始化参数列表\n",
    "    loss_hist = np.zeros(num_iter + 1)\n",
    "    theta = np.ones(num_features)\n",
    "    \n",
    "    theta_hist[0] = theta\n",
    "    loss_hist[0] = compute_square_loss(X , y , theta)\n",
    "    \n",
    "    for i in range(num_iter) :\n",
    "        if (check_gradient == True) :\n",
    "            if (gradient_check(X , y , theta) == False) :\n",
    "                return False\n",
    "        theta = theta - alpha * compute_square_loss_gradient(X , y , theta)\n",
    "        theta_hist[i + 1] = theta\n",
    "        loss_hist[i + 1] = compute_square_loss(X , y , theta)\n",
    "    \n",
    "    return theta_hist , loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_hist , loss_hist = batch_grad_descent(X_train , y_train)\n",
    "#print (loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Experiment1(X_train , y_train , X_test , y_test) :\n",
    "    '''\n",
    "    实验一主要讨论了学习率以及迭代次数对于模型收敛性的影响\n",
    "    现象：1.当学习率过大时,模型难以收敛\n",
    "         2.当学习率较小并且轮数较多的时候,虽然在training set上的loss减小,但是test set上的loss反而增加\n",
    "    '''\n",
    "    alpha_list = [0.1 , 0.05 ,0.01 , 0.005]\n",
    "    color_list = ['G' , 'B' , 'R' , 'Y']\n",
    "    plt.figure(figsize = (16 , 16))\n",
    "    num_iter = 2000\n",
    "    step_size = 100\n",
    "    plt.xlim(0 , num_iter)\n",
    "    plt.ylim(0 , 5)\n",
    "    for i in range(len(alpha_list)) :\n",
    "        theta_hist , loss_hist = batch_grad_descent(X_train , y_train , alpha = alpha_list[i] , num_iter = num_iter)\n",
    "        x_cord = [step_size * i for i in range(int(num_iter / step_size) + 1)]\n",
    "        y_cord = [loss_hist[step_size * i] for i in range(int(num_iter / step_size) + 1)]\n",
    "        plt.plot(x_cord , y_cord , label = 'alpha = '+  str(alpha_list[i]) , color = color_list[i])\n",
    "        #print (i , 'train_error =' , loss_hist[-1] , ', test_error =' , compute_square_loss(X_test , y_test , theta_hist[-1]))\n",
    "    #plt.show()\n",
    "Experiment1(X_train , y_train , X_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_grad_descent_Backtracking_line_search(X , y , num_iter = 1000 , gamma = 0.5 , epsilon = 0.25) :\n",
    "    '''\n",
    "    利用回溯线性搜索方法优化的梯度下降法,原来是每一次迭代中,先求出梯度下降方向,然后为了不一次越界,考虑找到一个合理的最大步长t*grad,\n",
    "                            这个t应该满足while循环里的条件 L(theta - t * grad) <= L(theta) - epsilon * <t * grad , grad>\n",
    "                                                                            = L(theta) - epsilon * t * |grad|^2\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        num_iter - 最大迭代次数 , 标量\n",
    "        gamma - 回溯时使用的放缩参数 , 标量 , 0 < gammar < 1\n",
    "        epsilon - 搜索停止参数 , 标量\n",
    "    Returns:\n",
    "        theta_hist - 迭代过程中储存的参数列表 , 二维numpy数组(num_iter + 1 , num_features) , 其中 theta_hist[0]为初始参数, \n",
    "                     theta_hist[-1]为迭代结束时的参数\n",
    "        loss_hist - 迭代过程中储存的loss列表,一维numpy数组(num_iter + 1)\n",
    "    '''\n",
    "    [num_instances , num_features] = X.shape\n",
    "    theta_hist = np.zeros((num_iter + 1 , num_features))\n",
    "    loss_hist = np.zeros(num_iter + 1)\n",
    "    theta = np.ones(num_features)\n",
    "    loss = compute_square_loss(X , y , theta)\n",
    "    \n",
    "    theta_hist[0] = theta\n",
    "    loss_hist[0] = loss\n",
    "    \n",
    "    for i in range(num_iter) :\n",
    "        grad = compute_square_loss_gradient(X , y , theta)\n",
    "        square_grad = np.sum(grad * grad)\n",
    "        t = 1.0\n",
    "        while ((compute_square_loss(X , y , theta - t * grad)) > (loss - epsilon * t * square_grad)) :\n",
    "            t = t * gamma\n",
    "        theta = theta - t * gamma * grad\n",
    "        loss = compute_square_loss(X , y , theta)\n",
    "        theta_hist[i + 1] = theta\n",
    "        loss_hist[i + 1] = loss\n",
    "    return theta_hist , loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18220359532076\n",
      "1.477647851866748\n"
     ]
    }
   ],
   "source": [
    "def Experiment2(X_train , y_train , X_test , y_test) :\n",
    "    '''\n",
    "    实验二讨论了朴素的梯度下降法和利用回溯线性搜索优化的梯度下降法的效率问题,发现后者可以更快更好地收敛.\n",
    "    '''\n",
    "    theta_hist , loss_hist = batch_grad_descent_Backtracking_line_search(X_train , y_train , num_iter = 200)\n",
    "    print (loss_hist[-1])\n",
    "    theta_hist , loss_hist = batch_grad_descent(X_train , y_train , num_iter = 200 , alpha = 0.1)\n",
    "    print (loss_hist[-1])\n",
    "Experiment2(X_train , y_train , X_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss(X , y , theta , lambda_reg = 1 , B = 0) :\n",
    "    '''\n",
    "    带正则的线性模型的平方损失函数  loss = |X * theta - y|^2 / m + lambda_reg * |theta|^2 + (B - 1) * theta[-1]^2\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        lambda_reg - 正则化项系数 , 标量\n",
    "        B - 消除bias term的正则化影响 , 标量  \n",
    "    Returns:\n",
    "        loss - 平方损失 , 标量\n",
    "    '''\n",
    "    y_ = np.matmul(X , theta)\n",
    "    square_loss = np.mean((y - y_) ** 2) / 2\n",
    "    reg_loss = lambda_reg * np.sum((theta) ** 2)\n",
    "    reg_loss += (B - 1) * lambda_reg * theta[-1] * theta[-1]\n",
    "    loss = square_loss + reg_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在解决回归问题时,不该对bias term加正则约束,因此需要调整.\n",
    "         #一种方法是在生成额外特征时的1可以换成一个充分大的B\n",
    "         #第二种方法在计算loss和grad的时候修改掉\n",
    "        \n",
    "def compute_regularized_square_loss_gradient(X , y , theta , lambda_reg = 1 , B = 0) :\n",
    "    ''' \n",
    "    计算平方损失函数关于参数theta的梯度   grad = X^T * (X * theta - y) / m + 2 * lambda_reg * (theta - (0,0,...,theta[-1]))\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        lambda_reg - 正则化系数 , 标量\n",
    "        B - 消除bias term的正则化影响 , 标量\n",
    "    Returns:\n",
    "        grad - 梯度向量 , 一维numpy数组(num_features)\n",
    "    '''\n",
    "    num_instances = X.shape[0]\n",
    "    y_ = np.matmul(X , theta)\n",
    "    grad_square = np.matmul(X.T , (y_ - y)) / num_instances\n",
    "    grad_reg = 2 * lambda_reg * theta\n",
    "    grad_reg[-1] += 2 * (B - 1) * lambda_reg * theta[-1]\n",
    "    grad = grad_square + grad_reg\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print (compute_square_loss(X_train , y_train , theta))\n",
    "# print (compute_regularized_square_loss(X_train , y_train , theta , lambda_reg = 0))\n",
    "theta = np.ones(X_train.shape[1])\n",
    "#print (compute_regularized_square_loss_gradient(X_train , y_train , theta))\n",
    "generic_gradient_check(X_train , y_train , theta , compute_regularized_square_loss , compute_regularized_square_loss_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(X , y , alpha = 0.0001 , lambda_reg = 1 ,\n",
    "                            num_iter = 1000 , B = 0 , \n",
    "                            objective_func = compute_regularized_square_loss , \n",
    "                            gradient_func = compute_regularized_square_loss_gradient):\n",
    "    [num_instances , num_features] = X.shape\n",
    "    theta = np.ones(num_features)\n",
    "    loss = objective_func(X , y , theta , lambda_reg , B)\n",
    "    \n",
    "    theta_hist = np.zeros((num_iter , num_instances , num_features))\n",
    "    loss_hist = np.zeros((num_iter , num_instances))\n",
    "    \n",
    "    for i in range(num_iter) :\n",
    "        for j in range(num_instances) :\n",
    "            X_feed = X[j].reshape(1 , -1)\n",
    "            y_feed = np.array(y[j])\n",
    "            grad = gradient_func(X_feed , y_feed , theta , lambda_reg , B)\n",
    "            theta -= alpha * grad\n",
    "            loss = objective_func(X , y  , theta , lambda_reg , B)\n",
    "            theta_hist[i][j] = theta\n",
    "            loss_hist[i][j] = loss\n",
    "    return theta_hist , loss_hist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.67515007 3.67507012 3.67507174 3.67508802 3.67511822 3.67516013\n",
      " 3.67518849 3.67519238 3.6752373  3.67530369 3.67538866 3.67558125\n",
      " 3.67550734 3.67522457 3.67533688 3.67563149 3.67566674 3.67564033\n",
      " 3.67559476 3.67556102 3.67554764 3.67556054 3.67544475 3.67527184\n",
      " 3.67525937 3.67550514 3.67521773 3.67508473 3.67507462 3.67508011\n",
      " 3.67508449 3.67509315 3.67507732 3.67507719 3.67508943 3.67516034\n",
      " 3.67515962 3.67512694 3.67509319 3.67509661 3.67508303 3.67508623\n",
      " 3.67507835 3.67507551 3.67511608 3.67509759 3.67510605 3.67512722\n",
      " 3.67524204 3.67526481 3.67527083 3.67529063 3.67520851 3.67508459\n",
      " 3.67509792 3.67509254 3.67511229 3.67511659 3.67513223 3.67513714\n",
      " 3.67511774 3.67512073 3.67512549 3.67507765 3.67513643 3.67549195\n",
      " 3.67551017 3.67557501 3.67580927 3.67553772 3.67532825 3.67541139\n",
      " 3.67544682 3.67547943 3.67534486 3.67517328 3.67518262 3.67518551\n",
      " 3.67514502 3.67514327 3.67520723 3.6751698  3.67509243 3.67512279\n",
      " 3.67512864 3.67513735 3.67518769 3.67518437 3.67518819 3.67513867\n",
      " 3.67513815 3.67508973 3.67531746 3.67573964 3.67528302 3.67533924\n",
      " 3.67536051 3.67538408 3.67512617 3.67513108]\n"
     ]
    }
   ],
   "source": [
    "theta_hist , loss_hist = stochastic_grad_descent(X_train , y_train , \n",
    "                                                 objective_func = compute_regularized_square_loss ,\n",
    "                                                 gradient_func = compute_regularized_square_loss_gradient)\n",
    "print (loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
