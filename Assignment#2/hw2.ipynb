{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_construction(num_instances = 150 , num_features = 75) :\n",
    "    '''\n",
    "    生成数据 , X随机生成.theta为真实参数,前十位均从{-10,10}中随机选取,其余位全为0. y=X*theta+epsilon,其中epsilon符合~N(0,0.1)\n",
    "    Args:\n",
    "        num_instances - 样本个数 , 标量\n",
    "        num_features - 特征个数 , 标量\n",
    "    Returns:\n",
    "        X - 输入数据 , 二维numpy矩阵(num_instances , num_features)\n",
    "        y - label , 一维numpy矩阵(num_instances)\n",
    "        theta - 用来生成label的模型真实参数 , 一维numpy矩阵(num_features)\n",
    "    '''\n",
    "    X = np.random.rand(num_instances , num_features)\n",
    "    theta = np.zeros(num_features)\n",
    "    for i in range(10) :\n",
    "        if (np.random.randint(0,2) % 2 == 0) :\n",
    "            theta[i] = 10.0 \n",
    "        else :\n",
    "            theta[i] = -10.0\n",
    "    mean = 0\n",
    "    std = 0.1\n",
    "    epsilon = np.random.normal(mean , std , num_instances)\n",
    "    y = np.matmul(X , theta) + epsilon\n",
    "    \n",
    "    return X , y , theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10. -10. -10. -10.  10.  10. -10. -10. -10. -10.]\n"
     ]
    }
   ],
   "source": [
    "X , y , true_theta = data_construction()\n",
    "print (true_theta[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(X , y , num_instances = 150 , num_train = 80 , num_valid = 20 , num_test = 50) :\n",
    "    '''\n",
    "    分别按照前80,20,50依次划分训练集,验证集,测试集.\n",
    "    Args:\n",
    "        X - 输入数据 , 二维numpy矩阵(num_instances , num_features)\n",
    "        y - 标签 , 一维numpy矩阵(num_instances)\n",
    "        num_instances - 总样本个数 , 标量\n",
    "        num_train - 训练样本个数 , 标量\n",
    "        num_valid - 验证集样本个数 , 标量\n",
    "        num_test - 训练集样本个数 , 标量\n",
    "    Returns:\n",
    "        X_train - 训练集输入 , 二维numpy矩阵(num_train , num_features)\n",
    "        y_train - 训练集标签 , 一维numpy矩阵(num_train)\n",
    "        X_valid - 验证集输入 , 二维numpy矩阵(num_valid , num_features)\n",
    "        y_valid - 验证集标签 , 一维numpy矩阵(num_valid)\n",
    "        X_test - 测试集输入 , 二维numpy矩阵(num_test , num_features)\n",
    "        y_test - 测试集标签 , 一维numpy矩阵(num_test)\n",
    "    '''\n",
    "    X_train = X[ : num_train]\n",
    "    y_train = y[ : num_train]\n",
    "    X_valid = X[num_train : num_train + num_valid]\n",
    "    y_valid = y[num_train : num_train + num_valid]\n",
    "    X_test = X[-num_test : ]\n",
    "    y_test = y[-num_test : ]\n",
    "    return X_train , y_train , X_valid , y_valid , X_test , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train , y_train , X_valid ,  y_valid , X_test , y_test = data_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_normlization(X_train , X_valid , X_test) :\n",
    "    '''\n",
    "    将每一维特征的数据利用线性变换[0,1]对训练数据和测试数据归一化,其中统计量来自于训练数据.\n",
    "    Args:\n",
    "        X_train - 训练集输入数据 , 二维numpy数组(num_train , num_features)\n",
    "        X_valid - 验证集输入数据 , 二维numpy数组(num_valid , num_features)\n",
    "        X_test - 测试集输入数据 , 二维numpy数组(num_test , num_features)\n",
    "    \n",
    "    Returns:\n",
    "        X_train - 经过归一化的训练集 , 二维numpy数组(num_train , num_features)\n",
    "        X_valid - 经过归一化的验证集 , 二维numpy数组(num_valid , num_features)\n",
    "        X_test - 经过归一化的测试集 , 二维numpy数组(num_test , num_features)\n",
    "    '''\n",
    "    maximal = np.max(X_train , axis = 0)\n",
    "    minimal = np.min(X_train , axis = 0)\n",
    "    X_train = (X_train - minimal + 1e-8) / (maximal - minimal + 1e-8)\n",
    "    X_valid = (X_valid - minimal + 1e-8) / (maximal - minimal + 1e-8)\n",
    "    X_test = (X_test - minimal + 1e-8) / (maximal - minimal + 1e-8)\n",
    "    return X_train , X_valid , X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train , X_valid , X_test = feature_normlization(X_train , X_valid , X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss(X , y , theta , lambda_reg = 1) :\n",
    "    '''\n",
    "    带正则的线性模型的平方损失函数  loss = |X * theta - y|^2 / m + lambda_reg * |theta|^2\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        lambda_reg - 正则化项系数 , 标量\n",
    "    Returns:\n",
    "        loss - 平方损失 , 标量\n",
    "    '''\n",
    "    y_predict = np.matmul(X , theta)\n",
    "    square_loss = np.mean((y_predict - y) ** 2) / 2\n",
    "    regularized_loss = lambda_reg * np.sum((theta)**2)\n",
    "    loss = square_loss + regularized_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss_gradient(X ,  y , theta , lambda_reg = 1) :\n",
    "    ''' \n",
    "    计算平方损失函数关于参数theta的梯度   grad = X^T * (X * theta - y) / m + 2 * lambda_reg *  theta\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        lambda_reg - 正则化系数 , 标量\n",
    "    Returns:\n",
    "        grad - 梯度向量 , 一维numpy数组(num_features)\n",
    "    '''\n",
    "    num_instances = X.shape[0]\n",
    "    y_predict = np.matmul(X , theta)\n",
    "    grad = np.matmul(X.T , y_predict - y) / num_instances + 2 * lambda_reg * theta\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X , y , alpha = 0.01 , num_iter = 20000 , lambda_reg = 1) :\n",
    "    '''\n",
    "    利用梯度下降法求解平方损失函数的线性模型的参数\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        alpha - 学习率/梯度步长 , 标量\n",
    "        num_iter - 最大迭代次数 , 标量\n",
    "    Returns:\n",
    "        theta_hist - 迭代过程中储存的参数列表 , 二维numpy数组(num_iter , num_features)\n",
    "        loss_hist - 迭代过程中储存的loss列表,一维numpy数组(num_iter)\n",
    "    '''\n",
    "    [num_instances , num_features] = X.shape\n",
    "    theta = np.ones(num_features)\n",
    "    \n",
    "    theta_hist = np.zeros((num_iter , num_features))\n",
    "    loss_hist = np.zeros(num_iter)\n",
    "    \n",
    "    for i in range(num_iter) :\n",
    "        grad = compute_regularized_square_loss_gradient(X , y , theta , lambda_reg)\n",
    "        theta -= alpha * grad\n",
    "        loss = compute_regularized_square_loss(X , y , theta , lambda_reg)\n",
    "        theta_hist[i] = theta\n",
    "        loss_hist[i] = loss\n",
    "    return theta_hist , loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Experiment1():\n",
    "    '''\n",
    "    实验1 Ridge regression,注意没有bias term.比较了手写的梯度下降法和sklearn包中的SMO求解的效率和精度的差距.\n",
    "    sklearn包的Ridge的效果更好,速度更快.\n",
    "    '''\n",
    "    lambda_reg = 1e-4\n",
    "    theta_hist , loss_hist = batch_gradient_descent(X , y , lambda_reg = lambda_reg)\n",
    "    model = linear_model.Ridge(alpha = lambda_reg , fit_intercept = False)\n",
    "    model.fit(X_train , y_train)\n",
    "    #print (model.coef_)\n",
    "    #print (compute_regularized_square_loss(X_valid , y_valid , model.coef_ , lambda_reg))\n",
    "    y_predict = model.predict(X_valid)\n",
    "    #print (np.mean((y_predict - y_valid)**2)/2 + lambda_reg * np.sum((model.coef_)**2))\n",
    "    y_predict = model.predict(X_train)\n",
    "    #print (np.mean((y_predict - y_train)**2)/2 + lambda_reg * np.sum((model.coef_)**2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Experiment1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_L1_square_loss(X , y , theta , lambda_reg = 1e-4) :\n",
    "    '''\n",
    "    带正则的线性模型的平方损失函数  loss = |X * theta - y|^2 / m + lambda_reg * |theta|\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        lambda_reg - 正则化项系数 , 标量\n",
    "    Returns:\n",
    "        loss - 平方损失 , 标量\n",
    "    '''\n",
    "    y_predict = np.matmul(X , theta)\n",
    "    square_loss = np.mean((y_predict - y)**2) / 2 \n",
    "    l1_loss = lambda_reg * np.sum(np.abs(theta))\n",
    "    loss = square_loss + l1_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sign(x) :\n",
    "    '''\n",
    "    符号函数\n",
    "    Args:\n",
    "        x - 待求变量 , 标量\n",
    "    Returns :\n",
    "        sgn - 符号 , {-1,0,1}\n",
    "    '''\n",
    "    if (x > 0) :\n",
    "        return 1\n",
    "    if (x < 0) :\n",
    "        return -1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def piecewise_function(a , c , lambda_reg) :\n",
    "    '''\n",
    "    用于梯度坐标下降中的参数更新的分段函数  f(a,c,lambda_reg) = sgn(c / a) * max(0 , |c / a| - lambda_reg / a)\n",
    "    Args:\n",
    "        a , b , c - 输入变量 , 标量\n",
    "    Returns:\n",
    "        f - 分段函数值 , 标量\n",
    "    \n",
    "    '''\n",
    "    if (c > lambda_reg) :\n",
    "        return (c - lambda_reg) / a\n",
    "    if (c < -lambda_reg) :\n",
    "        return (c + lambda_reg) / a\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Coordinate_Descent(X , y , theta , num_iter = 2000 ,  lambda_reg = 0.015) :\n",
    "    '''\n",
    "    梯度坐标下降法求解Lasso回归问题. 每一轮迭代中,依次选取坐标进行更新,即考虑argmin_{w_j} Loss(w1,w2,...wj,..wd),注意到有闭式解,\n",
    "    写出来是一个经典的分段二次函数(L1正则的绝对值导致的现象),因此闭式解是一个三段的分段函数.这个分段函数也解释了为什么L1正则使得参数更容易\n",
    "    变为0.\n",
    "    Args:\n",
    "        X - 输入特征数据 , 二维numpy数组(num_instances , num_features)\n",
    "        y - 标签label数据 , 一维numpy数组(num_instances)\n",
    "        theta - 模型参数数据 , 一维numpy数组(num_features)\n",
    "        num_iter - 最大迭代次数 , 标量\n",
    "        lambda_reg - 正则化项系数 , 标量\n",
    "    Returns:\n",
    "        theta_hist - 迭代过程中储存的参数列表 , 二维numpy数组(num_iter , num_features)\n",
    "        loss_hist - 迭代过程中储存的loss列表,一维numpy数组(num_iter)\n",
    "    '''\n",
    "    [num_instances , num_features] = X.shape\n",
    "    theta_hist = np.zeros((num_iter , num_features))\n",
    "    loss_hist = np.zeros(num_iter)\n",
    "    A = np.zeros(num_features)\n",
    "    C = np.zeros(num_features)\n",
    "    for step in range(num_iter) :\n",
    "        for j in range(num_features) :\n",
    "            A[j] = np.mean((X.T[j]) ** 2)\n",
    "            C[j] = np.mean((y * X.T[j] + theta[j] * X.T[j] * X.T[j])) - np.mean((np.matmul(X , theta) * X.T[j]))\n",
    "            theta[j] = piecewise_function(A[j]  , C[j] , lambda_reg)\n",
    "        theta_hist[step] = theta\n",
    "        loss_hist[step] = compute_L1_square_loss(X , y , theta , lambda_reg)\n",
    "    return theta_hist , loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Experiment2() :\n",
    "    '''\n",
    "    实验2验证了坐标梯度下降法来求解Lasso回归,选取不同大小的正则化系数lambda,观察系数的变化\n",
    "    Lasso回归调lambda超参数的两个技巧\n",
    "        1.当lambda充分大的时候,所有的w都会被压成0,因此lambda的范围[0,lambda_max]可以快速定下来\n",
    "        2.一个观察就是类似lipschitz连续,lambda_1和lambda_2很接近,则对应的最优解w_1和w_2也会很接近.所以当调节lambda的时候,\n",
    "          起点不需要从w=0开始计算,而是可以从上一个lambda对应的最优解出发.\n",
    "    '''\n",
    "    #theta = model.coef_\n",
    "    theta = np.ones(X_train.shape[1])\n",
    "    theta_hist , loss_hist = Coordinate_Descent(X_train , y_train , theta)\n",
    "    print (theta_hist[-1])\n",
    "    print (loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.43218721e+00 -9.77872923e+00 -9.01764132e+00 -9.65238249e+00\n",
      "  9.39663558e+00  8.87843382e+00 -9.77256200e+00 -9.61520256e+00\n",
      " -9.54441726e+00 -9.86219653e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.13962673e-01  0.00000000e+00\n",
      "  0.00000000e+00 -2.52905968e-02 -3.38649637e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -9.26580177e-02\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.84784174e-02 -3.94442189e-02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -6.83325097e-02\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.37567839e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -8.69937215e-02 -1.81378963e-02  0.00000000e+00 -1.09477341e-01\n",
      "  0.00000000e+00  0.00000000e+00 -2.22407597e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.20132011e-01\n",
      " -6.81010348e-02  0.00000000e+00  0.00000000e+00]\n",
      "1.4538561626598492\n"
     ]
    }
   ],
   "source": [
    "Experiment2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
